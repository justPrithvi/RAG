# RAG Workflow Explained ğŸ“š

A detailed guide on how the RAG system works, for developers coming from JavaScript.

## ğŸ”„ Complete Data Flow

### Part 1: Document Processing (Ingestion)

```
User uploads file
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Main App (React + NestJS)        â”‚
â”‚                                     â”‚
â”‚  1. Receive file upload             â”‚
â”‚  2. Extract text from PDF/DOCX      â”‚
â”‚     (using libraries like pdf-parse)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ HTTP POST /api/documents/process
           â”‚ {
           â”‚   "document_id": "doc_123",
           â”‚   "text": "Long document text...",
           â”‚   "metadata": {
           â”‚     "filename": "report.pdf",
           â”‚     "user_id": "user_456"
           â”‚   }
           â”‚ }
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python RAG Service                â”‚
â”‚   (app/api/routes/documents.py)     â”‚
â”‚                                     â”‚
â”‚   process_document() endpoint       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DocumentService                   â”‚
â”‚   (app/services/document_service.py)â”‚
â”‚                                     â”‚
â”‚   Orchestrates the pipeline:        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Chunking                    â”‚
â”‚ (app/utils/chunking.py)             â”‚
â”‚                                     â”‚
â”‚ Input:                              â”‚
â”‚   "This is a very long document     â”‚
â”‚    about Python and machine         â”‚
â”‚    learning. Python is a great      â”‚
â”‚    language. Machine learning       â”‚
â”‚    is powerful..."                  â”‚
â”‚                                     â”‚
â”‚ Output (chunks):                    â”‚
â”‚   [                                 â”‚
â”‚     "This is a very long document   â”‚
â”‚      about Python and machine       â”‚
â”‚      learning.",                    â”‚
â”‚     "Python is a great language.    â”‚
â”‚      Machine learning is powerful." â”‚
â”‚   ]                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Generate Embeddings         â”‚
â”‚ (app/utils/embeddings.py)           â”‚
â”‚                                     â”‚
â”‚ For each chunk, convert to vector:  â”‚
â”‚                                     â”‚
â”‚ Chunk 1:                            â”‚
â”‚   "This is a very long document..." â”‚
â”‚   â†’ [0.234, 0.876, 0.123, ...]      â”‚
â”‚      (768 numbers)                  â”‚
â”‚                                     â”‚
â”‚ Chunk 2:                            â”‚
â”‚   "Python is a great language..."   â”‚
â”‚   â†’ [0.456, 0.234, 0.789, ...]      â”‚
â”‚      (768 numbers)                  â”‚
â”‚                                     â”‚
â”‚ Uses: OpenAI API, Sentence          â”‚
â”‚       Transformers, or other        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Store in Vector DB          â”‚
â”‚ (app/utils/vector_store.py)         â”‚
â”‚                                     â”‚
â”‚ Store each chunk with:              â”‚
â”‚   - Chunk ID                        â”‚
â”‚   - Chunk text                      â”‚
â”‚   - Embedding vector                â”‚
â”‚   - Metadata                        â”‚
â”‚                                     â”‚
â”‚ Vector DB (ChromaDB/Pinecone)       â”‚
â”‚ enables fast similarity search      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Part 2: Querying (Retrieval + Generation)

```
User asks question: "What is Python used for?"
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Frontend (React)                 â”‚
â”‚                                     â”‚
â”‚    Sends query to backend           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ HTTP POST /api/documents/query
           â”‚ {
           â”‚   "question": "What is Python used for?",
           â”‚   "max_results": 5
           â”‚ }
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python RAG Service                â”‚
â”‚   (app/api/routes/documents.py)     â”‚
â”‚                                     â”‚
â”‚   query_documents() endpoint        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DocumentService                   â”‚
â”‚   query_documents()                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Embed the Question          â”‚
â”‚ (app/utils/embeddings.py)           â”‚
â”‚                                     â”‚
â”‚ Question:                           â”‚
â”‚   "What is Python used for?"        â”‚
â”‚   â†’ [0.445, 0.223, 0.789, ...]      â”‚
â”‚      (same 768 dimensions)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Similarity Search           â”‚
â”‚ (app/utils/vector_store.py)         â”‚
â”‚                                     â”‚
â”‚ Compare question vector with all    â”‚
â”‚ stored chunk vectors.               â”‚
â”‚                                     â”‚
â”‚ Find most similar chunks:           â”‚
â”‚   Chunk 2: similarity = 0.92        â”‚
â”‚   Chunk 1: similarity = 0.78        â”‚
â”‚   Chunk 5: similarity = 0.65        â”‚
â”‚                                     â”‚
â”‚ Return top 5 chunks                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Format Results              â”‚
â”‚                                     â”‚
â”‚ Return:                             â”‚
â”‚   {                                 â”‚
â”‚     "query": "What is Python...",   â”‚
â”‚     "results": [                    â”‚
â”‚       {                             â”‚
â”‚         "content": "Python is...",  â”‚
â”‚         "score": 0.92,              â”‚
â”‚         "metadata": {...}           â”‚
â”‚       }                             â”‚
â”‚     ]                               â”‚
â”‚   }                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPTIONAL STEP 4: LLM Generation     â”‚
â”‚                                     â”‚
â”‚ Send retrieved chunks to LLM:       â”‚
â”‚                                     â”‚
â”‚ Prompt:                             â”‚
â”‚   "Based on these documents:        â”‚
â”‚    [chunk 1, chunk 2, chunk 3]      â”‚
â”‚    Answer: What is Python used for?"â”‚
â”‚                                     â”‚
â”‚ LLM generates natural answer:       â”‚
â”‚   "Python is used for web dev,      â”‚
â”‚    machine learning, data science..." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§  Key Concepts Explained

### 1. Why Chunking?

**Problem**: Documents are long, but:
- Embedding models have token limits (usually 512-8192 tokens)
- LLMs have context limits
- Smaller chunks = more precise retrieval

**Solution**: Break into chunks
```python
# Instead of:
"Very long 50-page document..."  # Too big!

# We do:
[
  "Page 1 content...",    # Chunk 1
  "Page 2 content...",    # Chunk 2
  "Page 3 content...",    # Chunk 3
  ...
]
```

**Overlap**: Chunks overlap slightly to preserve context
```
Chunk 1: [____________________]
Chunk 2:           [____________________]
Chunk 3:                     [____________________]
                   â†‘â†‘â†‘â†‘      â†‘â†‘â†‘â†‘
                   Overlap    Overlap
```

### 2. What Are Embeddings?

**Think of embeddings as coordinates in space**

```
Text: "Python programming"
Embedding: [0.2, 0.8, 0.1, 0.5, ...] (768 numbers)
           Like coordinates: (x, y, z, ...)

Similar meanings = close in space!

"Python programming" â†’ [0.2, 0.8, 0.1, ...]
"Python coding"      â†’ [0.21, 0.79, 0.11, ...] â† Very close!
"Banana recipe"      â†’ [0.9, 0.1, 0.8, ...]   â† Far away!
```

**JavaScript Analogy**:
```javascript
// Like how we might hash or fingerprint content
const text = "Python programming";
const hash = generateHash(text);  // "a7b4c9..."

// But embeddings are smarter!
const embedding = generateEmbedding(text);  // [0.2, 0.8, ...]
// Similar text â†’ similar embeddings!
```

### 3. Vector Database

**Regular Database**:
```sql
SELECT * FROM documents WHERE text LIKE '%Python%'
-- Keyword matching only
```

**Vector Database**:
```python
# Find documents SIMILAR to query (semantically!)
query_embedding = embed("What is Python?")
results = vector_db.search(query_embedding, top_k=5)
# Returns documents about Python, even if they don't contain "Python"
# E.g., "This programming language..." might match!
```

**Popular Options**:
- **ChromaDB**: Local, easy to start, great for learning
- **Pinecone**: Cloud, scalable, production-ready
- **Weaviate, Qdrant**: Other production options

### 4. RAG vs Regular Search

**Regular Keyword Search**:
```
Query: "How do I learn Python?"
Database: Search for exact words "learn" and "Python"
Result: Documents containing these exact words
Problem: Misses synonyms, context, meaning
```

**RAG (Semantic Search + LLM)**:
```
Query: "How do I learn Python?"
Step 1: Convert to embedding (captures meaning)
Step 2: Find similar chunks (understands context)
        Matches: "Python tutorial", "Getting started with Python",
                "Python for beginners" â† No exact keywords!
Step 3: LLM generates answer using retrieved chunks
Result: Natural, contextual answer
```

## ğŸ› ï¸ Implementation Strategy

### Phase 1: Simple Text Chunking
```python
def simple_chunk(text, chunk_size=1000):
    # Split every 1000 characters
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i+chunk_size])
    return chunks
```

### Phase 2: Choose Embedding Provider

**Option A: OpenAI (Easiest)**
```python
import openai

response = openai.Embedding.create(
    model="text-embedding-ada-002",
    input="Your text here"
)
embedding = response['data'][0]['embedding']
# Returns: [0.234, 0.876, ...] (1536 dimensions)
```

**Option B: Sentence Transformers (Free, Local)**
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("Your text here")
# Returns: [0.234, 0.876, ...] (384 dimensions)
```

### Phase 3: Set Up ChromaDB (Simplest)
```python
import chromadb

# Initialize
client = chromadb.Client()
collection = client.create_collection("documents")

# Store
collection.add(
    embeddings=[[0.1, 0.2, ...]],  # Your embedding
    documents=["Chunk text"],
    ids=["chunk_1"]
)

# Search
results = collection.query(
    query_embeddings=[[0.15, 0.21, ...]],  # Query embedding
    n_results=5
)
```

## ğŸ“ Testing Strategy

### Test Locally (Without Real Implementation)

1. **Test Chunking**:
```bash
curl -X POST http://localhost:8000/api/documents/process \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "test_1",
    "text": "Short test text",
    "metadata": {"filename": "test.txt"}
  }'
```

2. **Test Query**:
```bash
curl -X POST http://localhost:8000/api/documents/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is this about?",
    "max_results": 5
  }'
```

3. **Use Swagger UI**: Visit `http://localhost:8000/docs`

## ğŸ¯ Learning Checkpoints

- [ ] âœ… Understand project structure
- [ ] Run the skeleton server
- [ ] Test endpoints with Swagger UI
- [ ] Implement simple chunking
- [ ] Choose and test embedding provider
- [ ] Set up vector database
- [ ] Wire everything together
- [ ] Test end-to-end flow
- [ ] Add error handling
- [ ] Optimize performance

## ğŸš€ When You're Ready

1. Pick one component to implement
2. Uncomment dependencies in `requirements.txt`
3. Test that component in isolation
4. Move to next component
5. Integrate everything

Don't try to build it all at once! ğŸ¯

